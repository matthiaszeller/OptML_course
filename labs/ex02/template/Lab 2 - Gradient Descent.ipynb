{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are running Python 3. Good job :)\n"
     ]
    }
   ],
   "source": [
    "# Check the Python version\n",
    "import sys\n",
    "if sys.version.startswith(\"3.\"):\n",
    "  print(\"You are running Python 3. Good job :)\")\n",
    "else:\n",
    "  print(\"This notebook requires Python 3.\\nIf you are using Google Colab, go to Runtime > Change runtime type and choose Python 3.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "  # Clone the entire repo to access the files.\n",
    "  !git clone -l -s https://github.com/epfml/OptML_course.git cloned-repo\n",
    "  %cd cloned-repo/labs/ex02/template/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "b, A = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape, A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares Estimation\n",
    "Least squares estimation is one of the fundamental machine learning algorithms. Given an $ n \\times d $ matrix $A$ and a $ n \\times 1$ vector $b$, the goal is to find a vector $x \\in \\mathbb{R}^d$ which minimizes the objective function $$f(x) = \\frac{1}{2n} \\sum_{i=1}^{n} (a_i^\\top x - b_i)^2 = \\frac{1}{2n} \\|Ax - b\\|^2 $$\n",
    "\n",
    "In this exercise, we will try to fit $x$ using Least Squares Estimation. \n",
    "\n",
    "One can see the function is $L$ smooth with $L =\\frac1n\\|A^T A\\|  = \\frac1n\\|A\\|^2$ (Lemma 2.3 for the first equality, and a few manipulations for the second)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Definition**: Let $f: \\text{dom}(f) \\to \\mathbb R$ be differentiable, $X \\subseteq \\text{dom}(f)$, $L \\in \\mathbb R_+$. The function $f$ is smooth with parameter $L$ over $X$ if:\n",
    "\n",
    "$$f(\\mathbf y) \\le f(\\mathbf x) + \\nabla f (\\mathbf x)^\\top (\\mathbf y - \\mathbf x) + \\frac L  2 \\Vert \\mathbf y - \\mathbf x \\Vert ^ 2, \\quad \\forall\\; \\mathbf x, \\mathbf y \\in X$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Objective Function\n",
    "Fill in the `calculate_objective` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_objective(Axmb):\n",
    "    \"\"\"Calculate the mean squared error for vector Axmb = Ax - b.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute mean squared error\n",
    "    # ***************************************************\n",
    "    return np.mean(Axmb ** 2) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute smoothness constant $L$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the spectral norm of A you can use np.linalg.norm(A, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_L(b, A):\n",
    "    \"\"\"Calculate the smoothness constant for f\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute ||A.T*A||\n",
    "    # ***************************************************\n",
    "    \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute L = smoothness constant of f\n",
    "    # ***************************************************\n",
    "    \n",
    "    return np.linalg.norm(A, 2) ** 2 / A.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "$$\\nabla f(\\mathbf x) = \\frac 1 n \\textstyle \\sum_{i=1}^n \\; \\left( \\mathbf a_i^\\top \\mathbf x - b_i  \\right) \\mathbf a_i = \\frac 1 n A^\\top \\left( A \\mathbf x - \\mathbf b \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(b, A, x):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute gradient and objective\n",
    "    # ***************************************************\n",
    "    Axmb = np.matmul(A, x) - b\n",
    "    grad = np.matmul(A.T, Axmb) / A.shape[0]\n",
    "    \n",
    "    return grad, Axmb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(b, A, initial_x, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store x and objective func. values\n",
    "    xs = [initial_x]\n",
    "    objectives = []\n",
    "    x = initial_x\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute gradient and objective function\n",
    "        # ***************************************************\n",
    "        grad, Axmb = compute_gradient(b, A, x)\n",
    "        obj = calculate_objective(Axmb)\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update x by a gradient descent step\n",
    "        # ***************************************************\n",
    "        x -= gamma * grad\n",
    "        # store x and objective function value\n",
    "        xs.append(x)\n",
    "        objectives.append(obj)\n",
    "        print(\"Gradient Descent({bi}/{ti}): objective={l}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=obj))\n",
    "\n",
    "    return objectives, xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function with a naive step size through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): objective=2792.2367127591674\n",
      "Gradient Descent(1/49): objective=2264.6350560300034\n",
      "Gradient Descent(2/49): objective=1837.27771407938\n",
      "Gradient Descent(3/49): objective=1491.118267099376\n",
      "Gradient Descent(4/49): objective=1210.7291150455724\n",
      "Gradient Descent(5/49): objective=983.6139018819913\n",
      "Gradient Descent(6/49): objective=799.6505792194906\n",
      "Gradient Descent(7/49): objective=650.640287862865\n",
      "Gradient Descent(8/49): objective=529.9419518639982\n",
      "Gradient Descent(9/49): objective=432.1762997049161\n",
      "Gradient Descent(10/49): objective=352.9861214560597\n",
      "Gradient Descent(11/49): objective=288.842077074486\n",
      "Gradient Descent(12/49): objective=236.88540112541122\n",
      "Gradient Descent(13/49): objective=194.80049360666067\n",
      "Gradient Descent(14/49): objective=160.71171851647276\n",
      "Gradient Descent(15/49): objective=133.09981069342058\n",
      "Gradient Descent(16/49): objective=110.73416535674829\n",
      "Gradient Descent(17/49): objective=92.61799263404369\n",
      "Gradient Descent(18/49): objective=77.943892728653\n",
      "Gradient Descent(19/49): objective=66.05787180528654\n",
      "Gradient Descent(20/49): objective=56.430194857359716\n",
      "Gradient Descent(21/49): objective=48.631776529538975\n",
      "Gradient Descent(22/49): objective=42.315057684004195\n",
      "Gradient Descent(23/49): objective=37.19851541912101\n",
      "Gradient Descent(24/49): objective=33.054116184565615\n",
      "Gradient Descent(25/49): objective=29.69715280457577\n",
      "Gradient Descent(26/49): objective=26.978012466783984\n",
      "Gradient Descent(27/49): objective=24.775508793172605\n",
      "Gradient Descent(28/49): objective=22.991480817547423\n",
      "Gradient Descent(29/49): objective=21.546418157290994\n",
      "Gradient Descent(30/49): objective=20.375917402483303\n",
      "Gradient Descent(31/49): objective=19.42781179108905\n",
      "Gradient Descent(32/49): objective=18.65984624585973\n",
      "Gradient Descent(33/49): objective=18.037794154223974\n",
      "Gradient Descent(34/49): objective=17.53393195999902\n",
      "Gradient Descent(35/49): objective=17.1258035826768\n",
      "Gradient Descent(36/49): objective=16.795219597045794\n",
      "Gradient Descent(37/49): objective=16.527446568684688\n",
      "Gradient Descent(38/49): objective=16.310550415712186\n",
      "Gradient Descent(39/49): objective=16.134864531804457\n",
      "Gradient Descent(40/49): objective=15.992558965839189\n",
      "Gradient Descent(41/49): objective=15.877291457407333\n",
      "Gradient Descent(42/49): objective=15.783924775577523\n",
      "Gradient Descent(43/49): objective=15.70829776329538\n",
      "Gradient Descent(44/49): objective=15.64703988334684\n",
      "Gradient Descent(45/49): objective=15.597421000588529\n",
      "Gradient Descent(46/49): objective=15.557229705554294\n",
      "Gradient Descent(47/49): objective=15.52467475657656\n",
      "Gradient Descent(48/49): objective=15.498305247904602\n",
      "Gradient Descent(49/49): objective=15.476945945880312\n",
      "Gradient Descent: execution time=0.070 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "\n",
    "gamma = 0.1\n",
    "\n",
    "# Initialization\n",
    "x_initial = np.zeros(A.shape[1])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gradient_objectives_naive, gradient_xs_naive = gradient_descent(b, A, x_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07b28a68d8ec4f37b6928ec5f2f86df8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "from grid_search import *\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    # Generate grid data for visualization (parameters to be swept and best combination)\n",
    "    grid_x0, grid_x1 = generate_w(num_intervals=10)\n",
    "    grid_objectives = grid_search(b, A, grid_x0, grid_x1)\n",
    "    obj_star, x0_star, x1_star = get_best_parameters(grid_x0, grid_x1, grid_objectives)\n",
    "    \n",
    "    fig = gradient_descent_visualization(\n",
    "        gradient_objectives_naive, gradient_xs_naive, grid_objectives, grid_x0, grid_x1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_xs_naive)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try doing gradient descent with a better learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We set $\\gamma := \\frac 1 L$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L= 2.2800993613523044, gamma = 0.4385773782274602\n",
      "Gradient Descent(0/49): objective=780.8686016504854\n",
      "Gradient Descent(1/49): objective=721.5052560260675\n",
      "Gradient Descent(2/49): objective=706.7843584315374\n",
      "Gradient Descent(3/49): objective=702.1659603718192\n",
      "Gradient Descent(4/49): objective=700.3195706210596\n",
      "Gradient Descent(5/49): objective=699.3927282552014\n",
      "Gradient Descent(6/49): objective=698.8265138747603\n",
      "Gradient Descent(7/49): objective=698.4252460378514\n",
      "Gradient Descent(8/49): objective=698.1121802865196\n",
      "Gradient Descent(9/49): objective=697.8540419775694\n",
      "Gradient Descent(10/49): objective=697.6346924802035\n",
      "Gradient Descent(11/49): objective=697.4451956987403\n",
      "Gradient Descent(12/49): objective=697.2798771098777\n",
      "Gradient Descent(13/49): objective=697.134694345026\n",
      "Gradient Descent(14/49): objective=697.0065306304226\n",
      "Gradient Descent(15/49): objective=696.8928669366339\n",
      "Gradient Descent(16/49): objective=696.7916143488984\n",
      "Gradient Descent(17/49): objective=696.701017217532\n",
      "Gradient Descent(18/49): objective=696.6195897256094\n",
      "Gradient Descent(19/49): objective=696.5460698345363\n",
      "Gradient Descent(20/49): objective=696.4793834248536\n",
      "Gradient Descent(21/49): objective=696.4186151939681\n",
      "Gradient Descent(22/49): objective=696.3629844974911\n",
      "Gradient Descent(23/49): objective=696.3118250564795\n",
      "Gradient Descent(24/49): objective=696.2645678090806\n",
      "Gradient Descent(25/49): objective=696.220726375205\n",
      "Gradient Descent(26/49): objective=696.1798847171167\n",
      "Gradient Descent(27/49): objective=696.141686656003\n",
      "Gradient Descent(28/49): objective=696.105826961754\n",
      "Gradient Descent(29/49): objective=696.0720437782061\n",
      "Gradient Descent(30/49): objective=696.0401121828639\n",
      "Gradient Descent(31/49): objective=696.0098387107206\n",
      "Gradient Descent(32/49): objective=695.9810566975377\n",
      "Gradient Descent(33/49): objective=695.9536223197188\n",
      "Gradient Descent(34/49): objective=695.9274112263599\n",
      "Gradient Descent(35/49): objective=695.9023156747344\n",
      "Gradient Descent(36/49): objective=695.8782420937702\n",
      "Gradient Descent(37/49): objective=695.8551090113992\n",
      "Gradient Descent(38/49): objective=695.8328452912637\n",
      "Gradient Descent(39/49): objective=695.8113886324363\n",
      "Gradient Descent(40/49): objective=695.7906842927655\n",
      "Gradient Descent(41/49): objective=695.7706840023488\n",
      "Gradient Descent(42/49): objective=695.7513450386715\n",
      "Gradient Descent(43/49): objective=695.7326294392002\n",
      "Gradient Descent(44/49): objective=695.7145033308639\n",
      "Gradient Descent(45/49): objective=695.6969363589233\n",
      "Gradient Descent(46/49): objective=695.6799012003625\n",
      "Gradient Descent(47/49): objective=695.6633731491596\n",
      "Gradient Descent(48/49): objective=695.6473297626892\n",
      "Gradient Descent(49/49): objective=695.6317505601199\n",
      "Gradient Descent: execution time=0.031 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: a better learning rate using the smoothness of f\n",
    "# ***************************************************\n",
    "L = calculate_L(b, A)\n",
    "gamma = 1 / L\n",
    "print(f'L= {L}, gamma = {gamma}')\n",
    "\n",
    "# Initialization\n",
    "x_initial = np.zeros(A.shape[1])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gradient_objectives, gradient_xs = gradient_descent(b, A, x_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time visualization with a better learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b88f6f6dde43309a38b857b8d6588d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot_figure(n_iter):\n",
    "    # Generate grid data for visualization (parameters to be swept and best combination)\n",
    "    grid_x0, grid_x1 = generate_w(num_intervals=10)\n",
    "    grid_objectives = grid_search(b, A, grid_x0, grid_x1)\n",
    "    obj_star, x0_star, x1_star = get_best_parameters(grid_x0, grid_x1, grid_objectives)\n",
    "    \n",
    "    fig = gradient_descent_visualization(\n",
    "        gradient_objectives, gradient_xs, grid_objectives, grid_x0, grid_x1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_xs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(gradient_objectives_naive, label='naive')\n",
    "plt.plot(gradient_objectives, label='gamma=1/L')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.title('Comparing naive stepsize vs exploit that f is smooth')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading more complex data\n",
    "The data is taken from https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"Concrete_Data.csv\",delimiter=\",\")\n",
    "\n",
    "A = data[:,:-1]\n",
    "b = data[:,-1]\n",
    "A, mean_A, std_A = standardize(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1030,), (1030, 8))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape, A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assuming bounded gradients\n",
    "Assume we are moving in a bounded region $\\|x\\| \\leq 25$ containing all iterates (and we assume $\\|x-x^\\star\\| \\leq 25$ as well, for simplicity). Then by $\\nabla f(x) = \\frac{1}{n}A^\\top (Ax - b)$, one can see that $f$ is Lipschitz over that bounded region, with Lipschitz constant $\\|\\nabla f(x)\\| \\leq \\frac{1}{n} (\\|A^\\top A\\|\\|x\\| + \\|A^\\top b\\|)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: Compute the bound on the gradient norm\n",
    "# ***************************************************\n",
    "grad_norm_bound = \n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the learning rate assuming bounded gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 50\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: Compute learning rate based on bounded gradient\n",
    "# ***************************************************\n",
    "gamma = \n",
    "raise NotImplementedError\n",
    "\n",
    "# Initialization\n",
    "x_initial = np.zeros(A.shape[1])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "bd_gradient_objectives, bd_gradient_xs = gradient_descent(b, A, x_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "\n",
    "# Averaging the iterates as is the case for bounded gradients case\n",
    "bd_gradient_objectives_averaged = []\n",
    "for i in range(len(bd_gradient_xs)):\n",
    "    if i > 0:\n",
    "        bd_gradient_xs[i] = (i * bd_gradient_xs[i-1] + bd_gradient_xs[i])/(i + 1)\n",
    "    grad, err = compute_gradient(b, A, bd_gradient_xs[i])\n",
    "    obj = calculate_objective(err)\n",
    "    bd_gradient_objectives_averaged.append(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent using smoothness\n",
    "Fill in the learning rate using smoothness of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 50\n",
    "\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: a better learning rate using the smoothness of f\n",
    "# ***************************************************\n",
    "gamma = \n",
    "raise NotImplementedError\n",
    "\n",
    "# Initialization\n",
    "x_initial = np.zeros(A.shape[1])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gradient_objectives, gradient_xs = gradient_descent(b, A, x_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Evolution of the Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.xlabel('Number of steps')\n",
    "plt.ylabel('Objective Function')\n",
    "#plt.yscale(\"log\")\n",
    "plt.plot(range(len(gradient_objectives)), gradient_objectives,'r', label='gradient descent with 1/L stepsize')\n",
    "plt.plot(range(len(bd_gradient_objectives)), bd_gradient_objectives,'b', label='gradient descent assuming bounded gradients')\n",
    "plt.plot(range(len(bd_gradient_objectives_averaged)), bd_gradient_objectives_averaged,'g', label='gradient descent assuming bounded gradients with averaged iterates')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "state": {
    "d2b2c3aea192430e81437f33ba0b0e69": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "e4a6a7a70ccd42ddb112989c04f2ed3f": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
